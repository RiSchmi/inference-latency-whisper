{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Preprocessing audio file to 16kHz frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and transform german audio corpus\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Step 1: Read the .parquet file\n",
    "audio_data = pd.read_parquet('data/train-00000-of-00084.parquet')\n",
    "\n",
    "audio_arrays = []\n",
    "\n",
    "# Step 2: Convert the audio bytes into a NumPy array with a sampling rate of 16000 Hz\n",
    "desired_sample_rate = 16000\n",
    "\n",
    "for audio_dict in audio_data['audio']:\n",
    "    \n",
    "    audio_bytes = audio_dict['bytes'] \n",
    "    audio_path = audio_dict['path'] \n",
    "    \n",
    "        # Convert binary audio to a NumPy array\n",
    "    audio_array, original_sample_rate = librosa.load(BytesIO(audio_bytes), sr=None)\n",
    "    \n",
    "    # Resample the audio to 16000 Hz\n",
    "    if original_sample_rate != desired_sample_rate:\n",
    "        audio_array = librosa.resample(y=audio_array, orig_sr=original_sample_rate, target_sr=desired_sample_rate)\n",
    "\n",
    "    audio_dict = {'path': audio_path, 'array': np.array(audio_array), 'sampling_rate': desired_sample_rate}\n",
    "    audio_arrays.append(audio_dict)\n",
    "\n",
    "# Step 3: remove binary audio representation and expand array representation\n",
    "\n",
    "audio_data = audio_data.drop('audio', axis = 1)\n",
    "audio_data = audio_data.rename({'transkription': 'labels'}, axis = 1)\n",
    "audio_data.insert(0, 'audio', audio_arrays) # insert dict with new audio representation \n",
    "\n",
    "\n",
    "# Step 4: save file\n",
    "batch_size = 5000\n",
    "for i in range(0, len(audio_data), batch_size):\n",
    "    audio_data[i:i+batch_size].to_parquet(f'data/german_{i//batch_size}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Step 1: Load the Parquet files\n",
    "train_dataset = Dataset.from_parquet(['data/german_0.parquet', 'data/german_1.parquet'])\n",
    "test_dataset = Dataset.from_parquet('data/german_2.parquet')\n",
    "\n",
    "# Step 2: transform audio from list to array\n",
    "train_dataset = train_dataset.with_format(\"np\", columns=[\"audio\"], output_all_columns=True)\n",
    "test_dataset = test_dataset.with_format(\"np\", columns=[\"audio\"], output_all_columns=True)\n",
    "\n",
    "# Step 3: combine to DatasetDict\n",
    "\n",
    "dataset_ger = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: load Whisper large-v3 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, pipeline\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # set device by checking for cuda enabled GPU or select CPU if not available\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# specify path to your (fine-tuned) model\n",
    "model_path = \"openai/whisper-large-v3\"# if you want to load pre-trained model or select path of fine-tuned model :'./whisper-finetuned-model/' (ideally .safetensors)\n",
    "processor = WhisperProcessor.from_pretrained(model_path, language=\"german\", task=\"transcribe\") # specify language and task\n",
    "\n",
    "# Load and initalize the model\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer = processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: run GPU inference latency test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_predict_time(sample):\n",
    "        'return GPU inference latency per audio sample'\n",
    "        start = time.time()\n",
    "        prediction = pipe(sample, generate_kwargs={\"language\": \"german\"})\n",
    "        end = time.time()\n",
    "        return prediction['text'], end - start\n",
    "\n",
    "def get_wer(predictions, references):\n",
    "        'return Word error rate (WER) based on ls of predicted and true transcript words'\n",
    "        len_ref = len(references)\n",
    "        len_pred = len(predictions)\n",
    "\n",
    "        if  len_pred > len_ref:\n",
    "            predictions = predictions[:len_ref]\n",
    "        else:\n",
    "            references = references[:len_pred]\n",
    "\n",
    "        return wer.compute(predictions= predictions, references= references),len_ref\n",
    "        \n",
    "\n",
    "# initialize placeholder to store latency and Word Error Rate per sample\n",
    "ls_latency = []\n",
    "ls_wer = []\n",
    "wer = load(\"wer\")\n",
    "\n",
    "# iterate through test data \n",
    "for n in range(len(dataset_ger['test'])):\n",
    "\n",
    "    # Step 1: define current sample\n",
    "    sample = dataset_ger['test'][n]['audio']['array']\n",
    "    sample_sentence = dataset_ger['test'][n]['labels']\n",
    "    \n",
    "\t# Step 2: track inference time and transcription of sample\n",
    "    text, latency = get_predict_time(sample)\n",
    "\n",
    "    # Step 3: calc error (WER)\n",
    "    wer_score, len_ref = get_wer(predictions = [x for x in text.split(' ') if x], \n",
    "                        references = [x for x in sample_sentence.split(' ') if x])\n",
    "\n",
    "    # Step 4: store results\n",
    "    time_per_token = latency/len_ref\n",
    "    ls_latency.append(time_per_token)  # add currrent latency to list\n",
    "    ls_wer.append(wer_score)\n",
    "    \n",
    "device_name= torch.cuda.get_device_name(device=device)\n",
    "pd.DataFrame({f'latency {device_name} ft': ls_latency}).to_csv(f'performance_results/{device_name}_latency_ft.csv', index= False)\n",
    "pd.DataFrame({f'WER {device_name} ft': ls_wer}).to_csv(f'performance_results/{device_name}_wer_ft.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
